# READ ME
## Description
This code was written for the SBLE seat position classifier project under Prof. Roberto Manduchi.
    The purpose of this project is to create a classifier which can determine the seat location (front, middle, or back) of 
    shuttle bus passengers using beacon measurements from two Bluetooth Low Energy beacons fixed to the roof above the front and
    back of the shuttle.
**NOTE: All sensitive data related to and used in this project has been omitted.**

## File Structure
- /data: this directory holds the SBLE and notification data files used for model training and testing. At the
    time of creation, there is only one notification file and several SBLE files.
- /matlab_archive: this directory holds MATLAB code created prior to this code base which parses the data, along
    with folders containing the data structures generated by that code. The code included here is meant to 
    replicate that process.
- /src:
    - /matlab.py: this file holds code used to parse the data structures saved from the matlab code for comparison
        with the data generated by the new python code.
    - /trip.py: this file defines the "trip" class, which holds the requisite data belonging to a trip recorded by 
        the SBLE system.
    - /parse.py: this file defines functions to parse the raw data from the files in /data and turn them into
        trip objects, as well as save and load these trips.
    - /filter.py: this file defines functions to take parsed trip objects and turn them into tagged data to be 
        used for model training and testing.
    - /test.py: this file defines functions to train and test each model type on generated trip objects.
    - /eval.py: this file defines functions to run full evaluations of each model type, returning dictionaries
        including model performance metrics for each metric considered in the project.
    - /random_forest.py: this file creates a wrapper class for training sklearn's random forest classifier to 
        be used on the given dataset.
    - /mlp.py: this file creates a wrapper class for training sklearn's multi-layer perceptron model to be used 
        on the given dataset.
    - /lstm.py: this file implements a PyTorch nn module to train a long short term memory model using the given 
        dataset.
    - /visualize.py: this file contains several methods to visualize the given data and performance metrics from 
        the trained models.

## Basic Usage
### Trip Parsing and Loading
- Upon initialization, save all relevant SBLE data and notification files in the data folder, and title each file
    with "_data" in the name for SBLE data and with "_notification" in the name for notification data.
- Call the function *get_trips_quick()* to get an array of trip objects, each holding the sble and notification data
    for a shuttle trip recorded by the system. Note that this function parses all the raw data from scratch, and
    takes a few minutes to execute.
- After doing so, use the function *save_tagged_data()* to save a csv of the compiled SBLE and notification files,
    respectively, but with the added column of "trip_idx", recording which trip each row belongs to or -1 if the row 
    is not a part of a completed trip.
- After doing this, use the function *get_trips_from_files()* to access the saved trips again without the need for 
    repeating the parsing process. Note that if additional data files are added, the parsing and recording process 
    will need to be repeated to include the new data.
- It is worth noting that several trip attributes like pre and post seat change are set and used during the parsing 
    process, and thus if trips are loaded from files these attributes may not be set accurately. Loaded trips will only 
    accurately set trip user, start, on_bus, seat, seat_time, and end, however these values may still be innacurate 
    depending on the nature of the trip (e.g. trips ending without a "collecting_data" = "false" notification will have 
    end times corresponding to the last beacon reading rather than the next "collecting_data" = "true" notification). 
    All other attributes require repetition of the parsing process for assignment.

### Data Tagging and Feature Extraction
- To get an array of DataFrames, each containing the SBLE data collected from mid-trip readings for the given trip 
    (i.e. after a user confirmed ridership status), use *get_tagged_dataset()*. Each DataFrame can be further filtered 
    and divided to separate X and y data as necessary later on.
    Note having each DataFrame separated by trip is necessary for two reasons:
        - Training/Testing splits should not divide individual trips, as patterns between datapoints in the same trip
            lead to overtraining to noise.
        - For the Long-Short Term Memory model, seequences need to be generated using several points from the same trip,
            so data needs to be separated by trip for sequence generation specific to that model.
- For training and testing splits, it is advised to use a Stratified Group KFold cross validation approach. However since data 
    is already separated by frame a Stratified KFold cross-validation object can be used, as the data is already grouped.

### Model Implementation, Training and Evaluation
- To assist in uniform usage of each model type, each wrapper takes in four similar parameters: a list of features to train 
    the model on, a scaler object to apply to these features, a PCA object if the implementation calls for principal component 
    analysis, and the features which are to be used in this PCA. If PCA or a scaler are not required, each can be set to None.
- Each model also takes in the default hyperparameters defined by the packages used to implement them.
- To train and test the models using the wrappers, the test.py file has functions for each model. These each take in an array 
    of trip objects, along with a wrapper object instantiated with the desired hyperparameters, and use the Stratified Group 
    KFold cross validation approach mentioned above to split training and testing data, train the models and return the testing 
    accuracy and confusion matrices.
- For a more comprehensive testing approach, the eval.py file contains functions to perform similar tests but ones that yield additional 
    metrics such as f1 score, as well as a more granular look at how the model performs at the split and trip level (i.e. how accurate 
    it was predicting seats for each trip and for each split).
- To get just the raw predictions, the eval.py file also includes functions to generate seat predictions for each trip using the given fold 
    each trip is included in the testing set.
- For an even more manual approach, each wrapper has fit, predict, and predict_proba functions to be used after defining the objects with 
    their desired hyperparemeters.

## Example Usage
### Parsing Trips from Scratch and Generating New Tagged Data & Notification Files
```python
trips = get_trips_quick()
sble = get_sble_data()
notif = get_notif_data()
sb_fn = "tagged_sble_data.csv"
not_fn = "tagged_notif_data.csv"
save_tagged_data(sble_data = sble, notif_data = notif, trips = trips, sble_filename = sb_fn, notif_filename = not_fn)
```

### Training and Testing each Model with Tagged Data
```python
sble = pd.read_csv("tagged_sble_data.csv")
notif = pd.read_csv("tagged_notif_data.csv")
trips = get_trips_from_files(sble_data = sble, notif_data = notif)

rf_model = RandomForest(features = rssi_features, scaler = RobustScaler(), n_estimators = 50, criterion = 'log_loss', max_features = 'sqrt', max_depth = 10, min_samples_split = 20, min_samples_leaf = 12, ccp_alpha = 0)
rf_acc, rf_cm = test_random_forest(trips, rf_model)

mlp_model = MLP(features = rssi_features, scaler = RobustScaler(), hidden_layer_sizes = (50, 50), batch_size = 16, activation = 'relu', learning_rate = 'adaptive', learning_rate_init = 0.001, alpha = 0.01, max_iter = 500, early_stopping = True, n_iter_no_change = 25)
mlp_acc, mlp_cm = test_mlp(trips, mlp_model)

lstm_model = SklearnLSTMWrapper(features = rssi_features, scaler = StandardScaler(), pca = PCA(n_components = 5), hidden_size = 50, lr = 0.001, num_epochs = 5, sub_sequence_length = 12, batch_size = 25, num_layers = 1, bidirectional = False, recurrent_dropout = 0.25, l2_lambda = 0.05)
lstm_acc, lstm_cm = test_lstm(trips, lstm_model)
```

### Retrieving all Predictions Made by each Model Type
```python
sble = pd.read_csv("tagged_sble_data.csv")
notif = pd.read_csv("tagged_notif_data.csv")
trips = get_trips_from_files(sble_data = sble, notif_data = notif)
data = get_tagged_dataset(trips)
X = [frame[all_features] for frame in data]
y = [frame.iloc[0]["seat"] if frame.shape[0] > 0 else -1 for frame in data]
cv = StratifiedKFold(n_splits = 5, random_state = 33, shuffle = True)

rf_model = RandomForest(features = rssi_features, scaler = RobustScaler(), n_estimators = 50, criterion = 'log_loss', max_features = 'sqrt', max_depth = 10, min_samples_split = 20, min_samples_leaf = 12, ccp_alpha = 0)
rf_preds, rf_true, row_split = get_rf_preds(rf_model, X, y, cv) 

mlp_model = MLP(features = rssi_features, scaler = RobustScaler(), hidden_layer_sizes = (50, 50), batch_size = 16, activation = 'relu', learning_rate = 'adaptive', learning_rate_init = 0.001, alpha = 0.01, max_iter = 500, early_stopping = True, n_iter_no_change = 25)
mlp_preds, mlp_true, row_split = get_mlp_preds(mlp_model, X, y, cv)

lstm_model = SklearnLSTMWrapper(features = rssi_features, scaler = StandardScaler(), pca = PCA(n_components = 5), hidden_size = 50, lr = 0.001, num_epochs = 5, sub_sequence_length = 12, batch_size = 25, num_layers = 1, bidirectional = False, recurrent_dropout = 0.25, l2_lambda = 0.05)
lstm_preds, lstm_true, row_split = get_lstm_preds(lstm_model, X, y, cv)
```

### Retrieving all Evaluation Metrics for each Model Type
```python
sble = pd.read_csv("tagged_sble_data.csv")
notif = pd.read_csv("tagged_notif_data.csv")
trips = get_trips_from_files(sble_data = sble, notif_data = notif)
data = get_tagged_dataset(trips)
X = [frame[all_features] for frame in data]
y = [frame.iloc[0]["seat"] if frame.shape[0] > 0 else -1 for frame in data]
cv = StratifiedKFold(n_splits = 5, random_state = 33, shuffle = True)

rf_model = RandomForest(features = rssi_features, scaler = RobustScaler(), n_estimators = 50, criterion = 'log_loss', max_features = 'sqrt', max_depth = 10, min_samples_split = 20, min_samples_leaf = 12, ccp_alpha = 0)
mlp_model = MLP(features = rssi_features, scaler = RobustScaler(), hidden_layer_sizes = (50, 50), batch_size = 16, activation = 'relu', learning_rate = 'adaptive', learning_rate_init = 0.001, alpha = 0.01, max_iter = 500, early_stopping = True, n_iter_no_change = 25)
lstm_model = SklearnLSTMWrapper(features = rssi_features, scaler = StandardScaler(), pca = PCA(n_components = 5), hidden_size = 50, lr = 0.001, num_epochs = 5, sub_sequence_length = 12, batch_size = 25, num_layers = 1, bidirectional = False, recurrent_dropout = 0.25, l2_lambda = 0.05)

rf_eval = analyze_rf(rf_model, X, y, cv)
mlp_eval = analyze_mlp(mlp_model, X, y, cv)
lstm_eval = analyze_lstm(lstm_model, X, y, cv)

# to get dictionary of evaluation for all models
rf_eval, mlp_eval, lstm_eval = full_suite(rf_model, mlp_model, lstm_model, trips)
```

### Generating Full Graphics Portfolio of Model Results
```python
sble = pd.read_csv("tagged_sble_data.csv")
notif = pd.read_csv("tagged_notif_data.csv")
trips = get_trips_from_files(sble_data = sble, notif_data = notif)

rf_model = RandomForest(features = rssi_features, scaler = RobustScaler(), n_estimators = 50, criterion = 'log_loss', max_features = 'sqrt', max_depth = 10, min_samples_split = 20, min_samples_leaf = 12, ccp_alpha = 0)
mlp_model = MLP(features = rssi_features, scaler = RobustScaler(), hidden_layer_sizes = (50, 50), batch_size = 16, activation = 'relu', learning_rate = 'adaptive', learning_rate_init = 0.001, alpha = 0.01, max_iter = 500, early_stopping = True, n_iter_no_change = 25)
lstm_model = SklearnLSTMWrapper(features = rssi_features, scaler = StandardScaler(), pca = PCA(n_components = 5), hidden_size = 50, lr = 0.001, num_epochs = 5, sub_sequence_length = 12, batch_size = 25, num_layers = 1, bidirectional = False, recurrent_dropout = 0.25, l2_lambda = 0.05)

rf_eval, mlp_eval, lstm_eval = full_suite(rf_model, mlp_model, lstm_model, trips)

generate_graphs(rf_eval, mlp_eval, lstm_eval, trips)
```
